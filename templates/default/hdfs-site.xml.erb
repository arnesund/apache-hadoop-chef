<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<property>
  <name>dfs.replication</name>
  <value><%= node[:hadoop][:num_replicas] %></value>
  <description>Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
</property>

<!-- 
<property>
<name>dfs.http.address</name>
<value><%= @addr1 %></value>
  <description> default port is 50070 but had to change it because it already in use
  </description>
</property>

<property>
<name>dfs.datanode.address</name>
<value><%= @addr2 %></value>
  <description> default port is 50070 but had to change it because it already in use
  </description>
</property>

<property>
<name>dfs.datanode.http.address</name>
<value><%= @addr3 %></value>
  <description> default port is 50070 but had to change it because it already in use
  </description>
</property>

<property>
<name>dfs.datanode.ipc.address</name>
<value><%= @addr4 %></value>
  <description> default port is 50070 but had to change it because it already in use
  </description>
</property>

<property>
<name>dfs.datanode.https.address</name>
<value><%= @addr5 %></value>
  <description> default port is 50070 but had to change it because it already in use
  </description>
</property>

-->

<property>
  <name>dfs.client.max.retires.on.failure</name>
  <value>1</value>
</property>

<property>
  <name>dfs.client.refresh.namenode.list</name>
  <value>60000</value>
</property>

<property>
  <name>dfs.namenode.name.dir</name>
  <value>file:<%= node[:hadoop][:nn][:name_dir] %></value>
</property>

<property>
  <name>dfs.datanode.data.dir</name>
  <value>file:<%= node[:hadoop][:dn][:data_dir] %></value>
</property>


<property>
  <name>dfs.client.block.write.locateFollowingBlock.retries</name>
  <value>10</value>
</property>

<property>
  <name>dfs.webhdfs.enabled</name>
  <value>true</value>
</property>

</configuration>
